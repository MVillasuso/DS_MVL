{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bit61836de9583b4678983c2459e76af05f",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio Base\n",
    "1. \n",
    "\n",
    "Crea un programa que pregunte al usuario qué tipo de algoritmo desea utilizar (entre regresión lineal, regresión logística y Knn).\n",
    "\n",
    "Se presupone que el usuario proporcionará un dataframe con los datos a entrenar, el nombre de la columna target que está en el dataframe.\n",
    "\n",
    "El programa debe contener, como mínimo, estas tres funciones tal que así:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "\"choose_model\" recibe:\n",
    "\n",
    "- 'option_user': la opción del usuario. \n",
    "- 'params': es un diccionario que puede contener ciertos parámetros necesarios para la creación de los modelos (por ejemplo, el valor k para el algoritmo Knn). Por defecto, su valor es None.\n",
    "\n",
    "Deberá crear el modelo necesario sin entrenar, retornándolo al final.\n",
    "\n",
    "----------------\n",
    "\n",
    "\"train_model\" recibe:\n",
    "\n",
    "- 'model': el modelo sin entrenar elegido por el usario\n",
    "- 'df': el dataframe tratado y limpio que contiene todos los datos del conjunto de entrenamiento y de test, incluyendo el target. \n",
    "- 'target_name': el nombre de la columna que representa el target.\n",
    "\n",
    "Retornará el modelo entrenado con los datos y la precisión del modelo.\n",
    "\n",
    "\n",
    "----------------\n",
    "\n",
    "\"main\": \n",
    "\n",
    "Es la función que ha de ser ejecutada cada vez que queremos lanzar el programa. \n",
    "\n",
    "Al final, mostrará la precisión del modelo entrenado y retornará el modelo entreado.\n",
    "\n",
    "----------------\n"
   ]
  },
  {
   "source": [
    "### Actualización 3 - Ejercicio base\n",
    "Añade al archivo \"3.regression_classification_exercise\" del CW8D4/exercises, todos los algoritmos que hemos visto. Estos son:  \n",
    "\n",
    "\n",
    "### Regresión:  \n",
    "Linear Regression  \n",
    "SVM (versión regresión SVR)  \n",
    "Polinominal Regression  \n",
    "Random Forest (versión regresión)  \n",
    "### Clasificación:    \n",
    "SVM (versión regresión SVC)  \n",
    "Knn  \n",
    "Random Forest (versión clasificación)  \n",
    "Xgboost \n",
    "Logistic regression  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "# Modelos de regresion\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "# Modelos de clasificacion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "# Pendientes\n",
    "#Gradient Booster Clasiffier\n",
    "#ExtraTrees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model(tipo, option_user, params=None):\n",
    "    if tipo ==1:\n",
    "        model_dicc={1:LinearRegression(), \n",
    "                    2:PolynomialFeatures(degree=params[\"degree\"]),\n",
    "                    3:SVR(kernel=params[\"kernel\"], C=params[\"C\"], gamma=params[\"gamma\"]),\n",
    "                    4:RandomForestRegressor(n_estimators=params[\"n_estimators\"]),\n",
    "                    5:XGBRegressor(n_estimators=params[\"n_estimators\"]),\n",
    "                    6:LGBMRegressor(n_estimators=params[\"n_estimators\"])\n",
    "                    }\n",
    "    else:\n",
    "        model_dicc={1:LogisticRegression(penalty= params[\"penalty\"]), \n",
    "                    2:KNeighborsClassifier(n_neighbors=params[\"k\"]), \n",
    "                    3:SVC(kernel=params[\"kernel\"], C=params[\"C\"], degree=params[\"degree\"]), \n",
    "                    4:RandomForestClassifier(n_estimators=params[\"n_estimators\"]), \n",
    "                    5:XGBClassifier(n_estimators=params[\"n_estimators\"])\n",
    "                    }\n",
    "\n",
    "    model = model_dicc[option_user]\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, df, target_name,poly):\n",
    "    lista_cols = df.columns.to_list()\n",
    "    lista_cols.remove(target_name)\n",
    "    X=df[lista_cols].values\n",
    "    y=df[target_name].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=tsize)\n",
    "    if poly == True:\n",
    "        X_poly = model.fit_transform(X_train, y_train)\n",
    "        model2=LinearRegression()\n",
    "        model_trained = model2.fit(X_poly,y_train) \n",
    "        y_pred = model2.predict(X_poly)\n",
    "        accuracy= r2_score(y_train, y_pred)\n",
    "    else:\n",
    "        model_trained = model.fit(X_train,y_train)      \n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = model.score(X_test, y_test)\n",
    "    \n",
    "    #Entrenarlo con todos los datos\n",
    "    #model_trained = model.fit(X,y)  \n",
    "    #accuracy = model.score(X, y)      \n",
    "    \n",
    "    return model_trained, accuracy\n",
    "\n",
    "\n",
    "def main():\n",
    "    global poly\n",
    "    tipo_modl = int(input (\"Indica el tipo: \" + \n",
    "                            \"(1):Regresión, \" + \n",
    "                            \"(2): Clasificación\"))\n",
    "    if tipo_modl ==1:\n",
    "        sel_model = int(input (\"Indica el modelo a utilizar: \" +\n",
    "                    \"(1): Regr.Lineal, \"+\n",
    "                    \"(2): Regresion Polinómica, \"+\n",
    "                    \"(3): SVR, \"+ \n",
    "                    \"(4): Random Forest Regressor\"+\n",
    "                    \"(5): XGBoosting Regressor (XGBoost)\" +\n",
    "                    \"(6): Light Gradient Boosting Regressor (LGBM)\"))\n",
    "    else:\n",
    "        sel_model = int(input (\"Indica el modelo a utilizar: \"+\n",
    "                    \"(1): Regr.Logística ,\"+ \n",
    "                    \"(2): KNN ,\"+\n",
    "                    \"(3): SVC ,\"+\n",
    "                    \"(4): Random Forest Classifier ,\"+\n",
    "                    \"(5): XGBoosting Regressor\"))  # OJO Este también puede usarse para regresión\n",
    "        \n",
    "    mod = choose_model(int(tipo_modl),int(sel_model),params)    #Selección del modelo \n",
    "    if tipo_modl==1 and sel_model==2:      #Regresión polinómica\n",
    "        poly =True\n",
    "    else:\n",
    "        poly=False\n",
    "\n",
    "    model_trained, accuracy = train_model(mod,df,target_col,poly=poly) # Entrenamiento del modelo \n",
    "    print(\"Accuracy: \", round(accuracy*100,2), \"%\")     #Precisión del modelo\n",
    "    return model_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables globales para la prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # opciones de kernel ['rbf','linear','poly']\n",
    "    params = {\"k\":5, \"degree\":3, \"kernel\":'rbf', \"C\":100, \"gamma\":1, \"epsilon\":.1, \"coef0\":1, \n",
    "            \"n_estimators\":10, \"penalty\": \"l2\"}\n",
    "    tsize=0.20\n",
    "    poly = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Prueba con datos para modelos de regresión\n",
    "    df = pd.read_csv(\"/Users/purbina/Desktop/THE_BRIDGE/DS_MVL/PROYECTOS/week8/day2/data/USA_Housing.csv\")\n",
    "    df.drop(['Address'], axis=1, inplace=True)\n",
    "    target_col = \"Price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Prueba con datos para Modelos de clasificación\n",
    "    df = pd.read_csv('/Users/purbina/Desktop/THE_BRIDGE/DS_MVL/PROYECTOS/week8/day4/data/iris.csv')\n",
    "    target_col = \"variety\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy:  87.44 %\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestRegressor(n_estimators=10)"
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "source": [
    "main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PARTE 2 PREDICCION\n",
    "2. Haz que el programa sea capaz de predecir un nuevo ejemplo del modelo elegido y entrenado con la siguiente función:\n",
    "----------------\n",
    "\n",
    "\"predict_new_data\" recibe:\n",
    "\n",
    "- 'model_trained': el modelo entrenado elegido por el usario\n",
    "- 'to_pred': los datos del nuevo ejemplo a predecir.\n",
    "\n",
    "Retornará y mostrará por pantalla la predicción.\n",
    "\n",
    "----------------\n",
    "\n",
    "Se presupone que el usuario proporcionará los datos del nuevo ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_data(model_trained, to_pred):\n",
    "    if poly==False:\n",
    "        y_pred = model_trained.predict(to_pred)\n",
    "    else:\n",
    "        polinominal_model = PolynomialFeatures(degree=params[\"degree\"]) \n",
    "        val_pred_poly = polinominal_model.fit_transform(val_pred)\n",
    "        y_pred=model_trained.predict(val_pred_poly)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de Prueba con Algoritmos de Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prueba para el modelo de regresión\n",
    "new_home = np.array([[100000.0, 20.0, 8.0, 9.0, 100000.0], [200000.0, 40.0, 16.0, 18.0, 110000.0]])\n",
    "val_pred = new_home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de Prueba con Algoritmos de Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prueba con modelos de clasificación\n",
    "val_pred=np.array([[15.7,2.8,9.5,0.1]])"
   ]
  },
  {
   "source": [
    "## Prueba de predicción"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy:  93.33 %\nValores Estimados con  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=10, n_jobs=0, num_parallel_tree=1,\n              objective='multi:softprob', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=None, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None) :  ['Versicolor']\n"
    }
   ],
   "source": [
    "mod_trained = main()\n",
    "val_estim = predict_new_data(mod_trained,val_pred)\n",
    "print(\"Valores Estimados con \", mod_trained, \": \",val_estim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}